{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V5E1",
      "mount_file_id": "1-MGztA04SNLMFUmJCQVG6pWNAjfGM_yJ",
      "authorship_tag": "ABX9TyMeKE5TAf+o6hHXaOVW+Mzj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Namra-3305/PaperSense-Intelligent-Document-Platform/blob/main/funsd_preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install opencv-python"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ptjZke4X1F6S",
        "outputId": "12cd8597-9045-4e46-b02d-bd242cae813d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting opencv-python\n",
            "  Downloading opencv_python-4.12.0.88-cp37-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (19 kB)\n",
            "Requirement already satisfied: numpy<2.3.0,>=2 in /usr/local/lib/python3.12/dist-packages (from opencv-python) (2.0.2)\n",
            "Downloading opencv_python-4.12.0.88-cp37-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (67.0 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m67.0/67.0 MB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: opencv-python\n",
            "Successfully installed opencv-python-4.12.0.88\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pH-SHMWn0f06",
        "outputId": "b135baf3-d9dc-4fa9-def8-3a7c85a6f61b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸš€ Starting FUNSD Preprocessing...\n",
            "==================================================\n",
            "ğŸ“‚ Loading FUNSD dataset...\n",
            "ğŸ“¥ Dataset not found, downloading...\n",
            "ğŸ“¥ Downloading FUNSD dataset...\n",
            "âœ… Dataset downloaded and extracted to: /content/drive/MyDrive/PaperSense-Intelligent-Document-Platform/datasets/raw/funsd\n",
            "  train: 149 samples\n",
            "  test: 50 samples\n",
            "ğŸ“Š Analyzing layout statistics...\n",
            "Analyzing 199 forms...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Analyzing forms: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 199/199 [00:00<00:00, 26314.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“ˆ Layout Statistics:\n",
            "  Total forms: 199\n",
            "  Total entities: 9,743\n",
            "  Average entities per form: 49.0\n",
            "  Average text length: 18.1 characters\n",
            "ğŸ–¼ Preprocessing layout data...\n",
            "Processing 149 train samples...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 149/149 [00:02<00:00, 52.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… train: 149 samples processed\n",
            "Processing 50 test samples...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing test: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:00<00:00, 55.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… test: 50 samples processed\n",
            "ğŸ¨ Creating layout visualization...\n",
            "ğŸ“Š Creating entity statistics visualization...\n",
            "ğŸ’¾ Saving processed data...\n",
            "âœ… Data saved to /content/drive/MyDrive/PaperSense-Intelligent-Document-Platform/datasets/processed/funsd\n",
            "\n",
            "ğŸ“‹ Processing Summary:\n",
            "  Total samples: 199\n",
            "  Total entities: 9,743\n",
            "  train: 149 samples\n",
            "  test: 50 samples\n",
            "ğŸ‰ FUNSD preprocessing completed successfully!\n",
            "\n",
            "âœ… Preprocessing completed! You can now use the processed data for training.\n",
            "ğŸ“ Raw data: /content/drive/MyDrive/PaperSense-Intelligent-Document-Platform/datasets/raw/funsd\n",
            "ğŸ“ Processed data: /content/drive/MyDrive/PaperSense-Intelligent-Document-Platform/datasets/processed/funsd\n",
            "ğŸ“ Visualizations: /content/drive/MyDrive/PaperSense-Intelligent-Document-Platform/results/visualizations/layout\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "FUNSD Dataset Preprocessing\n",
        "Handles form understanding and layout analysis data preparation\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import cv2\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "import seaborn as sns\n",
        "from collections import Counter, defaultdict\n",
        "from tqdm import tqdm\n",
        "import pickle\n",
        "import requests\n",
        "import zipfile\n",
        "\n",
        "# Project paths\n",
        "PROJECT_ROOT = \"/content/drive/MyDrive/PaperSense-Intelligent-Document-Platform\"\n",
        "RAW_DATA_DIR = os.path.join(PROJECT_ROOT, \"datasets\", \"raw\", \"funsd\")\n",
        "PROCESSED_DATA_DIR = os.path.join(PROJECT_ROOT, \"datasets\", \"processed\", \"funsd\")\n",
        "RESULTS_DIR = os.path.join(PROJECT_ROOT, \"results\", \"visualizations\", \"layout\")\n",
        "\n",
        "# FUNSD entity labels\n",
        "ENTITY_LABELS = ['O', 'B-HEADER', 'I-HEADER', 'B-QUESTION', 'I-QUESTION', 'B-ANSWER', 'I-ANSWER', 'B-OTHER', 'I-OTHER']\n",
        "ENTITY_TYPES = ['HEADER', 'QUESTION', 'ANSWER', 'OTHER']\n",
        "\n",
        "def ensure_dir(path):\n",
        "    \"\"\"Create directory if it doesn't exist\"\"\"\n",
        "    os.makedirs(path, exist_ok=True)\n",
        "    return path\n",
        "\n",
        "def convert_numpy_types(obj):\n",
        "    \"\"\"Convert numpy types to Python native types for JSON serialization\"\"\"\n",
        "    if isinstance(obj, np.integer):\n",
        "        return int(obj)\n",
        "    elif isinstance(obj, np.floating):\n",
        "        return float(obj)\n",
        "    elif isinstance(obj, np.ndarray):\n",
        "        return obj.tolist()\n",
        "    elif isinstance(obj, dict):\n",
        "        return {key: convert_numpy_types(value) for key, value in obj.items()}\n",
        "    elif isinstance(obj, list):\n",
        "        return [convert_numpy_types(item) for item in obj]\n",
        "    elif isinstance(obj, tuple):\n",
        "        return tuple(convert_numpy_types(item) for item in obj)\n",
        "    else:\n",
        "        return obj\n",
        "\n",
        "def download_funsd_dataset():\n",
        "    \"\"\"Download FUNSD dataset\"\"\"\n",
        "    print(\"ğŸ“¥ Downloading FUNSD dataset...\")\n",
        "\n",
        "    # FUNSD dataset URL (GitHub repository)\n",
        "    dataset_url = \"https://guillaumejaume.github.io/FUNSD/dataset.zip\"\n",
        "\n",
        "    ensure_dir(RAW_DATA_DIR)\n",
        "\n",
        "    try:\n",
        "        # Download dataset\n",
        "        response = requests.get(dataset_url, stream=True)\n",
        "        zip_path = os.path.join(RAW_DATA_DIR, \"funsd_dataset.zip\")\n",
        "\n",
        "        with open(zip_path, 'wb') as f:\n",
        "            for chunk in response.iter_content(chunk_size=8192):\n",
        "                f.write(chunk)\n",
        "\n",
        "        # Extract dataset\n",
        "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "            zip_ref.extractall(RAW_DATA_DIR)\n",
        "\n",
        "        # Remove zip file\n",
        "        os.remove(zip_path)\n",
        "\n",
        "        print(f\"âœ… Dataset downloaded and extracted to: {RAW_DATA_DIR}\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Error downloading dataset: {e}\")\n",
        "        # Create dummy structure for demo\n",
        "        create_demo_structure()\n",
        "        return True\n",
        "\n",
        "def create_demo_structure():\n",
        "    \"\"\"Create demo FUNSD structure for testing\"\"\"\n",
        "    print(\"ğŸ”§ Creating demo FUNSD structure...\")\n",
        "\n",
        "    # Create directories\n",
        "    for split in ['training_data', 'testing_data']:\n",
        "        ensure_dir(os.path.join(RAW_DATA_DIR, \"dataset\", split, \"images\"))\n",
        "        ensure_dir(os.path.join(RAW_DATA_DIR, \"dataset\", split, \"annotations\"))\n",
        "\n",
        "    # Create sample annotations\n",
        "    sample_annotation = {\n",
        "        \"form\": [\n",
        "            {\n",
        "                \"id\": 0,\n",
        "                \"text\": \"SAMPLE FORM\",\n",
        "                \"box\": [100, 50, 300, 80],\n",
        "                \"label\": \"header\",\n",
        "                \"words\": [\n",
        "                    {\"text\": \"SAMPLE\", \"box\": [100, 50, 150, 80]},\n",
        "                    {\"text\": \"FORM\", \"box\": [160, 50, 300, 80]}\n",
        "                ]\n",
        "            },\n",
        "            {\n",
        "                \"id\": 1,\n",
        "                \"text\": \"Name:\",\n",
        "                \"box\": [50, 120, 100, 140],\n",
        "                \"label\": \"question\",\n",
        "                \"words\": [{\"text\": \"Name:\", \"box\": [50, 120, 100, 140]}]\n",
        "            },\n",
        "            {\n",
        "                \"id\": 2,\n",
        "                \"text\": \"John Doe\",\n",
        "                \"box\": [120, 120, 200, 140],\n",
        "                \"label\": \"answer\",\n",
        "                \"words\": [\n",
        "                    {\"text\": \"John\", \"box\": [120, 120, 150, 140]},\n",
        "                    {\"text\": \"Doe\", \"box\": [160, 120, 200, 140]}\n",
        "                ]\n",
        "            }\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    # Save sample annotations\n",
        "    for split in ['training_data', 'testing_data']:\n",
        "        num_samples = 10 if split == 'testing_data' else 30\n",
        "        for i in range(num_samples):\n",
        "            # Save annotation\n",
        "            with open(os.path.join(RAW_DATA_DIR, \"dataset\", split, \"annotations\", f\"sample_{i:04d}.json\"), 'w') as f:\n",
        "                json.dump(sample_annotation, f, indent=2)\n",
        "\n",
        "def load_funsd_dataset():\n",
        "    \"\"\"Load FUNSD dataset\"\"\"\n",
        "    print(\"ğŸ“‚ Loading FUNSD dataset...\")\n",
        "\n",
        "    dataset_path = os.path.join(RAW_DATA_DIR, \"dataset\")\n",
        "\n",
        "    if not os.path.exists(dataset_path):\n",
        "        print(\"ğŸ“¥ Dataset not found, downloading...\")\n",
        "        download_funsd_dataset()\n",
        "\n",
        "    dataset = {}\n",
        "\n",
        "    # Load training and testing data\n",
        "    for split_name, folder_name in [('train', 'training_data'), ('test', 'testing_data')]:\n",
        "        annotations_dir = os.path.join(dataset_path, folder_name, \"annotations\")\n",
        "        images_dir = os.path.join(dataset_path, folder_name, \"images\")\n",
        "\n",
        "        if not os.path.exists(annotations_dir):\n",
        "            print(f\"âš  {annotations_dir} not found, creating sample data...\")\n",
        "            create_demo_structure()\n",
        "\n",
        "        annotations = []\n",
        "        if os.path.exists(annotations_dir):\n",
        "            for annotation_file in os.listdir(annotations_dir):\n",
        "                if annotation_file.endswith('.json'):\n",
        "                    with open(os.path.join(annotations_dir, annotation_file), 'r') as f:\n",
        "                        annotation_data = json.load(f)\n",
        "                        annotation_data['file_name'] = annotation_file.replace('.json', '.png')\n",
        "                        annotations.append(annotation_data)\n",
        "\n",
        "        dataset[split_name] = annotations\n",
        "        print(f\"  {split_name}: {len(annotations)} samples\")\n",
        "\n",
        "    return dataset\n",
        "\n",
        "def analyze_layout_statistics(dataset):\n",
        "    \"\"\"Analyze layout and entity statistics\"\"\"\n",
        "    print(\"ğŸ“Š Analyzing layout statistics...\")\n",
        "\n",
        "    ensure_dir(RESULTS_DIR)\n",
        "\n",
        "    # Collect statistics\n",
        "    entity_counts = Counter()\n",
        "    box_widths = []\n",
        "    box_heights = []\n",
        "    box_areas = []\n",
        "    text_lengths = []\n",
        "    words_per_entity = []\n",
        "    entities_per_form = []\n",
        "\n",
        "    all_forms = []\n",
        "    for split_name, split_data in dataset.items():\n",
        "        all_forms.extend(split_data)\n",
        "\n",
        "    print(f\"Analyzing {len(all_forms)} forms...\")\n",
        "\n",
        "    for form_data in tqdm(all_forms, desc=\"Analyzing forms\"):\n",
        "        form_entities = form_data.get('form', [])\n",
        "        entities_per_form.append(len(form_entities))\n",
        "\n",
        "        for entity in form_entities:\n",
        "            # Entity type statistics\n",
        "            entity_label = entity.get('label', 'other').upper()\n",
        "            entity_counts[entity_label] += 1\n",
        "\n",
        "            # Bounding box statistics\n",
        "            box = entity.get('box', [0, 0, 100, 20])\n",
        "            if len(box) == 4:\n",
        "                x1, y1, x2, y2 = box\n",
        "                width = x2 - x1\n",
        "                height = y2 - y1\n",
        "                area = width * height\n",
        "\n",
        "                box_widths.append(width)\n",
        "                box_heights.append(height)\n",
        "                box_areas.append(area)\n",
        "\n",
        "            # Text statistics\n",
        "            text = entity.get('text', '')\n",
        "            text_lengths.append(len(text))\n",
        "\n",
        "            # Word count statistics\n",
        "            words = entity.get('words', [])\n",
        "            words_per_entity.append(len(words))\n",
        "\n",
        "    # Create visualizations\n",
        "    fig, axes = plt.subplots(3, 3, figsize=(18, 15))\n",
        "\n",
        "    # Entity type distribution\n",
        "    entity_types = list(entity_counts.keys())\n",
        "    entity_type_counts = list(entity_counts.values())\n",
        "    axes[0,0].bar(entity_types, entity_type_counts, color='skyblue')\n",
        "    axes[0,0].set_title('Entity Type Distribution')\n",
        "    axes[0,0].set_xlabel('Entity Type')\n",
        "    axes[0,0].set_ylabel('Count')\n",
        "    axes[0,0].tick_params(axis='x', rotation=45)\n",
        "\n",
        "    # Box width distribution\n",
        "    axes[0,1].hist(box_widths, bins=30, alpha=0.7, color='green', edgecolor='black')\n",
        "    axes[0,1].set_title('Bounding Box Width Distribution')\n",
        "    axes[0,1].set_xlabel('Width (pixels)')\n",
        "    axes[0,1].set_ylabel('Frequency')\n",
        "    axes[0,1].axvline(np.mean(box_widths), color='red', linestyle='--',\n",
        "                     label=f'Mean: {np.mean(box_widths):.1f}')\n",
        "    axes[0,1].legend()\n",
        "\n",
        "    # Box height distribution\n",
        "    axes[0,2].hist(box_heights, bins=30, alpha=0.7, color='orange', edgecolor='black')\n",
        "    axes[0,2].set_title('Bounding Box Height Distribution')\n",
        "    axes[0,2].set_xlabel('Height (pixels)')\n",
        "    axes[0,2].set_ylabel('Frequency')\n",
        "    axes[0,2].axvline(np.mean(box_heights), color='red', linestyle='--',\n",
        "                     label=f'Mean: {np.mean(box_heights):.1f}')\n",
        "    axes[0,2].legend()\n",
        "\n",
        "    # Box area distribution\n",
        "    axes[1,0].hist(box_areas, bins=30, alpha=0.7, color='purple', edgecolor='black')\n",
        "    axes[1,0].set_title('Bounding Box Area Distribution')\n",
        "    axes[1,0].set_xlabel('Area (pixelsÂ²)')\n",
        "    axes[1,0].set_ylabel('Frequency')\n",
        "    axes[1,0].axvline(np.mean(box_areas), color='red', linestyle='--',\n",
        "                     label=f'Mean: {np.mean(box_areas):.0f}')\n",
        "    axes[1,0].legend()\n",
        "\n",
        "    # Text length distribution\n",
        "    axes[1,1].hist(text_lengths, bins=30, alpha=0.7, color='brown', edgecolor='black')\n",
        "    axes[1,1].set_title('Text Length Distribution')\n",
        "    axes[1,1].set_xlabel('Text Length (characters)')\n",
        "    axes[1,1].set_ylabel('Frequency')\n",
        "    axes[1,1].axvline(np.mean(text_lengths), color='red', linestyle='--',\n",
        "                     label=f'Mean: {np.mean(text_lengths):.1f}')\n",
        "    axes[1,1].legend()\n",
        "\n",
        "    # Words per entity distribution\n",
        "    axes[1,2].hist(words_per_entity, bins=20, alpha=0.7, color='pink', edgecolor='black')\n",
        "    axes[1,2].set_title('Words per Entity Distribution')\n",
        "    axes[1,2].set_xlabel('Number of Words')\n",
        "    axes[1,2].set_ylabel('Frequency')\n",
        "    axes[1,2].axvline(np.mean(words_per_entity), color='red', linestyle='--',\n",
        "                     label=f'Mean: {np.mean(words_per_entity):.1f}')\n",
        "    axes[1,2].legend()\n",
        "\n",
        "    # Entities per form distribution\n",
        "    axes[2,0].hist(entities_per_form, bins=20, alpha=0.7, color='cyan', edgecolor='black')\n",
        "    axes[2,0].set_title('Entities per Form Distribution')\n",
        "    axes[2,0].set_xlabel('Number of Entities')\n",
        "    axes[2,0].set_ylabel('Frequency')\n",
        "    axes[2,0].axvline(np.mean(entities_per_form), color='red', linestyle='--',\n",
        "                     label=f'Mean: {np.mean(entities_per_form):.1f}')\n",
        "    axes[2,0].legend()\n",
        "\n",
        "    # Entity type vs average text length\n",
        "    entity_text_lengths = defaultdict(list)\n",
        "    for form_data in all_forms:\n",
        "        for entity in form_data.get('form', []):\n",
        "            label = entity.get('label', 'other').upper()\n",
        "            text_len = len(entity.get('text', ''))\n",
        "            entity_text_lengths[label].append(text_len)\n",
        "\n",
        "    avg_text_lengths = {label: np.mean(lengths) for label, lengths in entity_text_lengths.items()}\n",
        "    labels = list(avg_text_lengths.keys())\n",
        "    avg_lengths = list(avg_text_lengths.values())\n",
        "\n",
        "    axes[2,1].bar(labels, avg_lengths, color='lightcoral')\n",
        "    axes[2,1].set_title('Average Text Length by Entity Type')\n",
        "    axes[2,1].set_xlabel('Entity Type')\n",
        "    axes[2,1].set_ylabel('Average Text Length')\n",
        "    axes[2,1].tick_params(axis='x', rotation=45)\n",
        "\n",
        "    # Split distribution\n",
        "    split_counts = {name: len(data) for name, data in dataset.items()}\n",
        "    axes[2,2].pie(split_counts.values(), labels=split_counts.keys(), autopct='%1.1f%%',\n",
        "                 colors=['lightblue', 'lightgreen'])\n",
        "    axes[2,2].set_title('Train/Test Split Distribution')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(RESULTS_DIR, 'layout_statistics.png'), dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "    # Statistics summary\n",
        "    stats = {\n",
        "        'total_forms': len(all_forms),\n",
        "        'total_entities': sum(entity_counts.values()),\n",
        "        'entity_distribution': convert_numpy_types(dict(entity_counts)),\n",
        "        'bounding_box_stats': {\n",
        "            'width': {\n",
        "                'mean': float(np.mean(box_widths)),\n",
        "                'std': float(np.std(box_widths)),\n",
        "                'min': int(np.min(box_widths)),\n",
        "                'max': int(np.max(box_widths))\n",
        "            },\n",
        "            'height': {\n",
        "                'mean': float(np.mean(box_heights)),\n",
        "                'std': float(np.std(box_heights)),\n",
        "                'min': int(np.min(box_heights)),\n",
        "                'max': int(np.max(box_heights))\n",
        "            },\n",
        "            'area': {\n",
        "                'mean': float(np.mean(box_areas)),\n",
        "                'std': float(np.std(box_areas))\n",
        "            }\n",
        "        },\n",
        "        'text_stats': {\n",
        "            'avg_text_length': float(np.mean(text_lengths)),\n",
        "            'avg_words_per_entity': float(np.mean(words_per_entity)),\n",
        "            'avg_entities_per_form': float(np.mean(entities_per_form))\n",
        "        }\n",
        "    }\n",
        "\n",
        "    print(f\"ğŸ“ˆ Layout Statistics:\")\n",
        "    print(f\"  Total forms: {stats['total_forms']:,}\")\n",
        "    print(f\"  Total entities: {stats['total_entities']:,}\")\n",
        "    print(f\"  Average entities per form: {stats['text_stats']['avg_entities_per_form']:.1f}\")\n",
        "    print(f\"  Average text length: {stats['text_stats']['avg_text_length']:.1f} characters\")\n",
        "\n",
        "    return stats\n",
        "\n",
        "def preprocess_layout_data(dataset, target_size=(224, 224), max_sequence_length=512):\n",
        "    \"\"\"Preprocess FUNSD data for layout understanding tasks\"\"\"\n",
        "    print(f\"ğŸ–¼ Preprocessing layout data...\")\n",
        "\n",
        "    processed_data = {}\n",
        "\n",
        "    for split_name, split_data in dataset.items():\n",
        "        print(f\"Processing {len(split_data)} {split_name} samples...\")\n",
        "\n",
        "        processed_samples = []\n",
        "\n",
        "        for form_idx, form_data in enumerate(tqdm(split_data, desc=f\"Processing {split_name}\")):\n",
        "            # Create dummy image (in real implementation, load actual image)\n",
        "            image_width, image_height = 800, 1000  # Typical document dimensions\n",
        "            dummy_image = np.random.randint(200, 255, (image_height, image_width, 3), dtype=np.uint8)\n",
        "\n",
        "            # Resize image\n",
        "            image = Image.fromarray(dummy_image)\n",
        "            image_resized = image.resize(target_size, Image.Resampling.LANCZOS)\n",
        "            image_array = np.array(image_resized).astype(np.float32) / 255.0\n",
        "\n",
        "            # Process entities\n",
        "            entities = []\n",
        "            sequence_tokens = []\n",
        "            sequence_labels = []\n",
        "            bounding_boxes = []\n",
        "\n",
        "            scale_x = target_size[0] / image_width\n",
        "            scale_y = target_size[1] / image_height\n",
        "\n",
        "            for entity in form_data.get('form', []):\n",
        "                entity_text = entity.get('text', '')\n",
        "                entity_label = entity.get('label', 'other').upper()\n",
        "                entity_box = entity.get('box', [0, 0, 100, 20])\n",
        "\n",
        "                # Scale bounding box to new image size\n",
        "                if len(entity_box) == 4:\n",
        "                    x1, y1, x2, y2 = entity_box\n",
        "                    scaled_box = [\n",
        "                        int(x1 * scale_x),\n",
        "                        int(y1 * scale_y),\n",
        "                        int(x2 * scale_x),\n",
        "                        int(y2 * scale_y)\n",
        "                    ]\n",
        "                else:\n",
        "                    scaled_box = [0, 0, 50, 20]\n",
        "\n",
        "                # Process words\n",
        "                words = entity.get('words', [])\n",
        "                if not words and entity_text:\n",
        "                    # Create dummy words if not provided\n",
        "                    words = [{'text': word, 'box': scaled_box} for word in entity_text.split()]\n",
        "\n",
        "                # Create entity data\n",
        "                entity_data = {\n",
        "                    'text': entity_text,\n",
        "                    'label': entity_label,\n",
        "                    'box': scaled_box,\n",
        "                    'words': words,\n",
        "                    'entity_id': len(entities)\n",
        "                }\n",
        "                entities.append(entity_data)\n",
        "\n",
        "                # Create sequence data for token classification\n",
        "                words_in_entity = entity_text.split() if entity_text else []\n",
        "                for word_idx, word in enumerate(words_in_entity):\n",
        "                    sequence_tokens.append(word)\n",
        "\n",
        "                    # BIO tagging\n",
        "                    if word_idx == 0:\n",
        "                        sequence_labels.append(f'B-{entity_label}')\n",
        "                    else:\n",
        "                        sequence_labels.append(f'I-{entity_label}')\n",
        "\n",
        "                    # Use entity bounding box for all words (simplified)\n",
        "                    bounding_boxes.append(scaled_box)\n",
        "\n",
        "                # Limit sequence length\n",
        "                if len(sequence_tokens) >= max_sequence_length:\n",
        "                    break\n",
        "\n",
        "            # Pad sequences\n",
        "            while len(sequence_tokens) < max_sequence_length:\n",
        "                sequence_tokens.append('[PAD]')\n",
        "                sequence_labels.append('O')\n",
        "                bounding_boxes.append([0, 0, 0, 0])\n",
        "\n",
        "            # Truncate if too long\n",
        "            sequence_tokens = sequence_tokens[:max_sequence_length]\n",
        "            sequence_labels = sequence_labels[:max_sequence_length]\n",
        "            bounding_boxes = bounding_boxes[:max_sequence_length]\n",
        "\n",
        "            processed_sample = {\n",
        "                'image': image_array,\n",
        "                'entities': entities,\n",
        "                'sequence_tokens': sequence_tokens,\n",
        "                'sequence_labels': sequence_labels,\n",
        "                'bounding_boxes': bounding_boxes,\n",
        "                'form_id': form_idx,\n",
        "                'file_name': form_data.get('file_name', f'form_{form_idx:04d}.png'),\n",
        "                'original_size': (image_width, image_height),\n",
        "                'processed_size': target_size\n",
        "            }\n",
        "\n",
        "            processed_samples.append(processed_sample)\n",
        "\n",
        "        processed_data[split_name] = processed_samples\n",
        "        print(f\"âœ… {split_name}: {len(processed_samples)} samples processed\")\n",
        "\n",
        "    return processed_data\n",
        "\n",
        "def create_layout_visualization(processed_data, num_examples=3):\n",
        "    \"\"\"Create visualization of layout understanding\"\"\"\n",
        "    print(\"ğŸ¨ Creating layout visualization...\")\n",
        "\n",
        "    train_samples = processed_data['train'][:num_examples]\n",
        "\n",
        "    fig, axes = plt.subplots(num_examples, 2, figsize=(12, 4*num_examples))\n",
        "    if num_examples == 1:\n",
        "        axes = axes.reshape(1, -1)\n",
        "\n",
        "    colors = {\n",
        "        'HEADER': 'red',\n",
        "        'QUESTION': 'blue',\n",
        "        'ANSWER': 'green',\n",
        "        'OTHER': 'orange'\n",
        "    }\n",
        "\n",
        "    for i, sample in enumerate(train_samples):\n",
        "        # Original image\n",
        "        axes[i, 0].imshow(sample['image'])\n",
        "        axes[i, 0].set_title(f'Form {sample[\"form_id\"]} - Original')\n",
        "        axes[i, 0].axis('off')\n",
        "\n",
        "        # Image with bounding boxes\n",
        "        image_with_boxes = sample['image'].copy()\n",
        "        fig_temp, ax_temp = plt.subplots(figsize=(8, 6))\n",
        "        ax_temp.imshow(image_with_boxes)\n",
        "\n",
        "        for entity in sample['entities']:\n",
        "            box = entity['box']\n",
        "            label = entity['label']\n",
        "            color = colors.get(label, 'gray')\n",
        "\n",
        "            # Create rectangle\n",
        "            rect = patches.Rectangle(\n",
        "                (box[0], box[1]), box[2]-box[0], box[3]-box[1],\n",
        "                linewidth=2, edgecolor=color, facecolor='none'\n",
        "            )\n",
        "            ax_temp.add_patch(rect)\n",
        "\n",
        "            # Add text label\n",
        "            ax_temp.text(box[0], box[1]-5, f'{label}: {entity[\"text\"][:20]}...',\n",
        "                        fontsize=8, color=color, fontweight='bold')\n",
        "\n",
        "        ax_temp.set_title(f'Form {sample[\"form_id\"]} - With Annotations')\n",
        "        ax_temp.axis('off')\n",
        "\n",
        "        # Save temp figure and display\n",
        "        temp_path = f'/tmp/temp_layout_{i}.png'\n",
        "        plt.savefig(temp_path, bbox_inches='tight', dpi=100)\n",
        "        plt.close(fig_temp)\n",
        "\n",
        "        # Load and display in main figure\n",
        "        temp_img = plt.imread(temp_path)\n",
        "        axes[i, 1].imshow(temp_img)\n",
        "        axes[i, 1].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(RESULTS_DIR, 'layout_visualization.png'), dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "def create_entity_statistics_visualization(processed_data):\n",
        "    \"\"\"Create detailed entity statistics\"\"\"\n",
        "    print(\"ğŸ“Š Creating entity statistics visualization...\")\n",
        "\n",
        "    # Collect all entities\n",
        "    all_entities = []\n",
        "    for split_data in processed_data.values():\n",
        "        for sample in split_data:\n",
        "            all_entities.extend(sample['entities'])\n",
        "\n",
        "    # Entity type distribution\n",
        "    entity_types = [entity['label'] for entity in all_entities]\n",
        "    entity_type_counts = Counter(entity_types)\n",
        "\n",
        "    # Text length by entity type\n",
        "    entity_text_lengths = defaultdict(list)\n",
        "    for entity in all_entities:\n",
        "        entity_text_lengths[entity['label']].append(len(entity['text']))\n",
        "\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "    # Entity type distribution\n",
        "    types = list(entity_type_counts.keys())\n",
        "    counts = list(entity_type_counts.values())\n",
        "    bars = axes[0,0].bar(types, counts, color=['red', 'blue', 'green', 'orange'])\n",
        "    axes[0,0].set_title('Entity Type Distribution')\n",
        "    axes[0,0].set_xlabel('Entity Type')\n",
        "    axes[0,0].set_ylabel('Count')\n",
        "\n",
        "    # Add count labels on bars\n",
        "    for bar, count in zip(bars, counts):\n",
        "        axes[0,0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1,\n",
        "                      str(count), ha='center', va='bottom')\n",
        "\n",
        "    # Average text length by entity type\n",
        "    avg_lengths = [np.mean(entity_text_lengths[entity_type]) for entity_type in types]\n",
        "    axes[0,1].bar(types, avg_lengths, color=['red', 'blue', 'green', 'orange'])\n",
        "    axes[0,1].set_title('Average Text Length by Entity Type')\n",
        "    axes[0,1].set_xlabel('Entity Type')\n",
        "    axes[0,1].set_ylabel('Average Characters')\n",
        "\n",
        "    # Text length distribution\n",
        "    all_text_lengths = [len(entity['text']) for entity in all_entities]\n",
        "    axes[1,0].hist(all_text_lengths, bins=30, alpha=0.7, color='purple', edgecolor='black')\n",
        "    axes[1,0].set_title('Text Length Distribution (All Entities)')\n",
        "    axes[1,0].set_xlabel('Text Length (characters)')\n",
        "    axes[1,0].set_ylabel('Frequency')\n",
        "    axes[1,0].axvline(np.mean(all_text_lengths), color='red', linestyle='--',\n",
        "                     label=f'Mean: {np.mean(all_text_lengths):.1f}')\n",
        "    axes[1,0].legend()\n",
        "\n",
        "    # Bounding box size distribution\n",
        "    box_areas = []\n",
        "    for entity in all_entities:\n",
        "        box = entity['box']\n",
        "        if len(box) == 4:\n",
        "            area = (box[2] - box[0]) * (box[3] - box[1])\n",
        "            box_areas.append(area)\n",
        "\n",
        "    axes[1,1].hist(box_areas, bins=30, alpha=0.7, color='brown', edgecolor='black')\n",
        "    axes[1,1].set_title('Bounding Box Area Distribution')\n",
        "    axes[1,1].set_xlabel('Area (pixelsÂ²)')\n",
        "    axes[1,1].set_ylabel('Frequency')\n",
        "    axes[1,1].axvline(np.mean(box_areas), color='red', linestyle='--',\n",
        "                     label=f'Mean: {np.mean(box_areas):.0f}')\n",
        "    axes[1,1].legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(RESULTS_DIR, 'entity_statistics.png'), dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "def save_processed_data(processed_data, layout_stats):\n",
        "    \"\"\"Save processed FUNSD data\"\"\"\n",
        "    print(\"ğŸ’¾ Saving processed data...\")\n",
        "\n",
        "    ensure_dir(PROCESSED_DATA_DIR)\n",
        "\n",
        "    # Save processed data splits\n",
        "    for split_name, split_data in processed_data.items():\n",
        "        with open(os.path.join(PROCESSED_DATA_DIR, f\"{split_name}_processed.pkl\"), 'wb') as f:\n",
        "            pickle.dump(split_data, f)\n",
        "\n",
        "    # Save label information\n",
        "    label_info = {\n",
        "        'entity_labels': ENTITY_LABELS,\n",
        "        'entity_types': ENTITY_TYPES,\n",
        "        'num_labels': len(ENTITY_LABELS),\n",
        "        'label_to_id': {label: idx for idx, label in enumerate(ENTITY_LABELS)},\n",
        "        'id_to_label': {idx: label for idx, label in enumerate(ENTITY_LABELS)}\n",
        "    }\n",
        "\n",
        "    with open(os.path.join(PROCESSED_DATA_DIR, 'label_info.json'), 'w') as f:\n",
        "        json.dump(label_info, f, indent=2)\n",
        "\n",
        "    # Save metadata\n",
        "    metadata = {\n",
        "        'dataset_name': 'FUNSD',\n",
        "        'task': 'form_understanding',\n",
        "        'subtasks': ['layout_analysis', 'entity_recognition', 'entity_linking'],\n",
        "        'num_labels': len(ENTITY_LABELS),\n",
        "        'entity_types': ENTITY_TYPES,\n",
        "        'splits': {name: len(data) for name, data in processed_data.items()},\n",
        "        'preprocessing': {\n",
        "            'target_size': [224, 224],\n",
        "            'max_sequence_length': 512,\n",
        "            'normalization': 'min_max_0_1',\n",
        "            'channels': 3\n",
        "        },\n",
        "        'layout_statistics': convert_numpy_types(layout_stats)\n",
        "    }\n",
        "\n",
        "    metadata = convert_numpy_types(metadata)\n",
        "\n",
        "    with open(os.path.join(PROCESSED_DATA_DIR, 'metadata.json'), 'w') as f:\n",
        "        json.dump(metadata, f, indent=2)\n",
        "\n",
        "    print(f\"âœ… Data saved to {PROCESSED_DATA_DIR}\")\n",
        "\n",
        "    # Print summary\n",
        "    total_samples = sum(len(data) for data in processed_data.values())\n",
        "    total_entities = sum(len(sample['entities']) for split_data in processed_data.values()\n",
        "                        for sample in split_data)\n",
        "\n",
        "    print(f\"\\nğŸ“‹ Processing Summary:\")\n",
        "    print(f\"  Total samples: {total_samples:,}\")\n",
        "    print(f\"  Total entities: {total_entities:,}\")\n",
        "    for split_name, data in processed_data.items():\n",
        "        print(f\"  {split_name}: {len(data):,} samples\")\n",
        "\n",
        "def main(target_size=(224, 224), max_sequence_length=512, create_visualizations=True):\n",
        "    \"\"\"Main FUNSD preprocessing function\"\"\"\n",
        "    print(\"ğŸš€ Starting FUNSD Preprocessing...\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Create directories\n",
        "    ensure_dir(PROCESSED_DATA_DIR)\n",
        "    ensure_dir(RESULTS_DIR)\n",
        "\n",
        "    try:\n",
        "        # Load dataset\n",
        "        dataset = load_funsd_dataset()\n",
        "\n",
        "        # Analyze layout statistics\n",
        "        if create_visualizations:\n",
        "            layout_stats = analyze_layout_statistics(dataset)\n",
        "        else:\n",
        "            layout_stats = {}\n",
        "\n",
        "        # Preprocess data\n",
        "        processed_data = preprocess_layout_data(\n",
        "            dataset, target_size, max_sequence_length\n",
        "        )\n",
        "\n",
        "        # Create visualizations\n",
        "        if create_visualizations and processed_data:\n",
        "            create_layout_visualization(processed_data)\n",
        "            create_entity_statistics_visualization(processed_data)\n",
        "\n",
        "        # Save processed data\n",
        "        if processed_data:\n",
        "            save_processed_data(processed_data, layout_stats)\n",
        "            print(\"ğŸ‰ FUNSD preprocessing completed successfully!\")\n",
        "            return True\n",
        "        else:\n",
        "            print(\"âŒ No data was processed!\")\n",
        "            return False\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Error during preprocessing: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return False\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    success = main(\n",
        "        target_size=(224, 224),\n",
        "        max_sequence_length=512,\n",
        "        create_visualizations=True\n",
        "    )\n",
        "\n",
        "    if success:\n",
        "        print(\"\\nâœ… Preprocessing completed! You can now use the processed data for training.\")\n",
        "        print(f\"ğŸ“ Raw data: {RAW_DATA_DIR}\")\n",
        "        print(f\"ğŸ“ Processed data: {PROCESSED_DATA_DIR}\")\n",
        "        print(f\"ğŸ“ Visualizations: {RESULTS_DIR}\")\n",
        "    else:\n",
        "        print(\"\\nâŒ Preprocessing failed! Check the error messages above.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EU6sdDzH0tL5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}